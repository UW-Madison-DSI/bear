{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP author search with OpenAlex data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal is to return at author (and institution level) from a given search query.\n",
    "\n",
    "### Groupby and aggregate\n",
    "\n",
    "Borrowing/Stealing? the design language from pandas, let's think about `groupby` and `aggregate`.\n",
    "\n",
    "- groupby: defines the return object level (`author` for now)\n",
    "- aggregate: what formula we will use to `reduce` multiple records into a single metric. For example:\n",
    "    - basic count of hits\n",
    "    - custom `reranker` borrowing from retrival augmented generation (RAG) field, we weights the things that people like to see to obtain a score. More specifically, we can obtain the `cited_by_count` in each relevant paper and sum all within an author.\n",
    "    - `reranker` can also sources form multiple underlying metrics like what we do in [faculty search](https://github.com/UW-Madison-DSI/faculty-search/blob/eff2ecfedcf5e817e70e3f3541b91d4cceeabb27/api/core.py#L387)\n",
    "\n",
    "TODOs:\n",
    "\n",
    "1. Make a MVP groupby aggregate interface for search with base hit counts\n",
    "1. Implement faculty search `reranker` into aggregate\n",
    "\n",
    "\n",
    "Mock Interface \n",
    "\n",
    "```python\n",
    "search_results = search(\"effect of fungicide on corn\")\n",
    "search_results.groupby(\"author\").aggregate(\"count\")  # Hit Counts, return authors\n",
    "search_results.groupby(\"author\").aggregate(\"reranker_v0\")  # Fully custom reranker, return author\n",
    "search_results.groupby(\"institution\").aggregate(\"count\")  # Hit Counts, return institute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what we have now, without pulling a lot of author data, we should make reranker_v0 as follow:\n",
    "\n",
    "1. Get a list of relevant articles w.r.t. search query\n",
    "2. group by author\n",
    "3. weight by sum of similarity\n",
    "\n",
    "This will be significantly better than the vanilla count without putting too much work into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bear.search import _search, SearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = _search(\"climate change\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SearchResults.from_raw(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bear import search\n",
    "\n",
    "search(\"Higgs field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlmodel import SQLModel\n",
    "from pathlib import Path\n",
    "from bear.db import init, ENGINE\n",
    "from bear.ingest import ingest\n",
    "\n",
    "SQLModel.metadata.drop_all(ENGINE)\n",
    "init()\n",
    "# ingest(Path(\"local_data/test_authors.parquet\"))\n",
    "# ingest(Path(\"local_data/test_articles.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlmodel import Session, text, Index\n",
    "from bear.db import ENGINE, Work\n",
    "from bear.settings import CONFIG\n",
    "\n",
    "with Session(ENGINE) as session:\n",
    "    session.connection().execute(text(\"DROP INDEX IF EXISTS work_index;\"))\n",
    "    session.commit()\n",
    "\n",
    "index = Index(\n",
    "    \"work_index\",\n",
    "    Work.embedding,\n",
    "    postgresql_using=\"hnsw\",\n",
    "    postgresql_with={\n",
    "        \"m\": CONFIG.HNSW_M,\n",
    "        \"ef_construction\": CONFIG.HNSW_EF_CONSTRUCTION,\n",
    "    },\n",
    "    postgresql_ops={\"embedding\": \"vector_ip_ops\"},\n",
    ")\n",
    "index.create(bind=ENGINE, checkfirst=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
